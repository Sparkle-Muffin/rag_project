modele mistral – kompendium (stan na sierpień 2025) opisy poszczególnych modeli mixtral 8×22b kolejny model moe złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld) oferuje 64k tokenów kontekstu i licencję apache 2.0. wersja bf16 wymaga około 263 gb vram, natomiast kwantyzacja int4 redukuje wymóg do około 65 gb artykuł aime podaje, że w praktyce potrzebne są 4× gpu 80 gb (a100/h100) model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli.
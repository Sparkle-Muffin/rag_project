pllum – polski model językowy dostępne modele pllum pllum‑8×7b (base/instruct/chat)  mixture‑of‑experts 8×7b (około 45 mld aktywnych parametrów); bazą jest mixtral 8×7b model moe działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci.  długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i rag.  apache 2.0 dla modeli open; wersje „nc” – licencja cc‑by‑nc‑4.0  orientacyjne wymagania vram na podstawie mixtral 8×7b: 4‑bitowa kwantyzacja (~22,5 gb vram), 8‑bit ~45 gb, pełna fp16 ~90 gb (z kontekstem ok. 32 k). modele „nc” mogą korzystać z rozszerzonego korpusu.
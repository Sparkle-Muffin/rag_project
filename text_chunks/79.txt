modele gpt – szczegółowy przegląd dla rag (stan na sierpień 2025) 1. ewolucja serii gpt gpt‑2 (luty 2019)  1,5 mld parametrów (wersja pełna); udostępniono także warianty 124 m, 355 m i 774 m  używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż gpt‑1). pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitach  wagi modelu pełnego mają ok. 5 gb i pochłaniają dużo ram podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć cpu w 100%  dla fp32 potrzeba ~6 gb vram, co potwierdzają przewodniki dotyczące uruchamiania gpt‑2  licencja mit; openai w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje.  model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu.
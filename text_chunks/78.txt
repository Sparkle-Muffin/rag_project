modele gpt – szczegółowy przegląd dla rag (stan na sierpień 2025) 1. ewolucja serii gpt gpt‑1 (czerwiec 2018)  ~117 mln parametrów  model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie bookscorpus; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjami  wagi mają ~0,5 gb; do uruchomienia w fp16 wystarcza konsumencki gpu.  wydany na licencji mit; wagi udostępniono publicznie poprzez github.  pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię gpt.
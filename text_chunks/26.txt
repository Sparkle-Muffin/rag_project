modele mistral – kompendium (stan na sierpień 2025) opisy poszczególnych modeli mixtral 8×7b model typu mixture‑of‑experts (moe) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrów dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. model oferuje kontekst 32k tokenów, licencję apache 2.0 oraz możliwość uruchamiania lokalnego. dyskusje na huggingface podają, że wersja 4‑bitowa wymaga ~22.5 gb vram, 8‑bitowa ~45 gb, a pełna półprecyzja ~90 gb artykuł aime sugeruje minimalną konfigurację 4× rtx a6000 48 gb dla trybu bf16 model sprawdza się w długim rag, generowaniu kodu i zadań wymagających dłuższego kontekstu.
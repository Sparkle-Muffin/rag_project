modele gpt – szczegółowy przegląd dla rag (stan na sierpień 2025) 1. ewolucja serii gpt gpt‑oss (sierpień 2025) – open‑weights  gpt‑oss‑120b: 117 mld parametrów, 5,1 mld aktywnych parametrów; gpt‑oss‑20b: 21 mld parametrów, 3,6 mld aktywnych  modele korzystają z kwantyzacji mxfp4 i obsługują 128 k tokenów dzięki pozycjonowaniu rope; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowego  gpt‑oss‑120b zmieści się na jednej karcie h100 80 gb, a gpt‑oss‑20b można uruchomić na gpu z 16 gb vram  pliki wag wymagają 80 gb (120b) lub 16 gb (20b) pamięci vram dzięki 4‑bitowej kwantyzacji  licencja apache 2.0 z niewielkim uzupełnieniem dotyczącym bezpiecznego użycia  modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędzia  dzięki otwartym wagom umożliwiają lokalny rag, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vllm czy llama.cpp.
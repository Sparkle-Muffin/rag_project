
# Modele Mistral – kompendium (stan na sierpień 2025)

## Wprowadzenie
**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.

## Podział na rodziny
- **Modele otwarte** – wagi są dostępne na HuggingFace i można je uruchamiać lokalnie. Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.). Wymagają znacznej pamięci VRAM podczas inferencji; tabela 1 podaje minimalne wartości.
- **Modele badawcze (Premier models)** – Mistral Large, Mixtral 8×22B, Pixtral Large, series Magistral Medium i Mistral Medium 3 mają wagi udostępnione tylko na zasadzie licencji badawczej (Mistral Research License) i są przeznaczone do testów niekomercyjnych. Samodzielne użycie wymaga licencji komercyjnej.
- **Modele komercyjne/API only** – Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych. Użytkownik wchodzi z nimi w relację usługową.

## Tabela 1 – skrócone parametry i wymagania VRAM
Poniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarów typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAM

| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB) | Licencja | Przykładowe zastosowania |
|---|---|---|---|---|---|
| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |
| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |
| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |
| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |
| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |
| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |
| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8) | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |
| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8) | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |
| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |
| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |
| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |
| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |
| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |
| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |
| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |
| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |
| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |

## Opisy poszczególnych modeli
### Mistral 7B
Pierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAM  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAM Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.

### Mixtral 8×7B
Model typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrów dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GB Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16 Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.

### Mixtral 8×22B
Kolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld) Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GB Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100) Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli.

### Codestral 22B (seria 25.01/25.08)
Codestral to open‑weight model przeznaczony do generowania kodu, wytrenowany na 80+ językach programowania z kontekstem 32k. Oferuje tryb *fill‑in‑the‑middle* (FIM) do uzupełniania brakujących fragmentów kodu i osiąga wysokie wyniki w HumanEval i MBPP.  Wersja 22 mld parametrów wymaga ok. 60 GB VRAM  Licencja MNPL pozwala na badania i projekty niekomercyjne do zastosowań komercyjnych konieczne jest wykupienie licencji.  W lipcu 2025 r. wprowadzono wersję 25.08, która zwiększyła odsetek akceptowanych uzupełnień kodu o 30 % i zmniejszyła liczbę „runaway generations” o 50 %  Codestral jest dostępny w stacku Mistral Code oraz w usłudze Codestral Embed (embedding do wyszukiwania kodu).  

### Codestral Mamba 7B
Pierwszy model Mistral w architekturze Mamba2, specjalizowany w generowaniu kodu. Ma 7.285 mld parametrów i korzysta z liniowej złożoności obliczeniowej, co teoretycznie pozwala na nieskończony kontekst. Udostępniony w licencji Apache 2.0 sprawdza się jako lokalny asystent programistyczny o niskiej latencji. Minimalne wymagania VRAM są porównywalne z Mistral 7B (około 16 GB).  

### Mathstral 7B
Model o 7 mld parametrach z kontekstem 32k przeznaczony do zadań matematycznych i STEM. Współpraca z projektem Numina zaowocowała wysokimi wynikami: 56,6 % na zbiorze MATH i 63,47 % na MMLU Model, dostępny na licencji Apache 2.0, zachowuje zgodność z architekturą Mistral 7B, dzięki czemu może pełnić rolę matematycznego eksperta w RAG.

### Mistral NeMo 12B
Model 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiece  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAM NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.

### Mistral Large 2 i Mistral Large 24.11
Mistral Large 2, ogłoszony w lipcu 2024 r., to flagowy model Mistrala z 123 mld parametrów i kontekstem 128k. Blog „Large Enough” podkreśla, że model znacząco poprawia generowanie kodu, matematyczne rozumowanie i obsługę wielu języków Model wydany jest w licencji badawczej; w pełnej precyzji wymaga ok. 233 GB VRAM, a kwantyzacja int4 redukuje wymagania do ~58 GB  Wersja 24.11 (Mistral Large 2.1) z listopada 2024 r. wprowadziła nowe podpowiedzi systemowe, lepsze wywoływanie funkcji i ulepszone wnioskowanie długokontekstowe

### Pixtral 12B i Pixtral Large
Pixtral 12B to pierwszy multimodalny model Mistrala. Składa się z 12 mld parametrów w dekoderze (na bazie Mistral NeMo) i 400 mln parametrów w enkoderze obrazu; model przyjmuje kilka obrazów oraz tekst, obsługuje zmienne rozmiary obrazów i ma 128k kontekstu Jest dostępny w licencji Apache 2.0; w precyzji bf16 wymaga 28 GB VRAM, w fp8 około 16 GB artykuł Ori podaje, że do uruchomienia na vLLM wystarczy 24 GB VRAM Pixtral Large (124 mld parametrów + 1 mld w enkoderze obrazu) ma kontekst 128k i przewyższa konkurencję na benchmarkach MathVista, DocVQA i VQA v2 Udostępniony jest w licencji badawczej; wymaga ok. 250 GB VRAM

### Mistral Small 3, 3.1 i 3.2 (24 B parametrów)
Seria Mistral Small to 24‑miliardowe modele zoptymalizowane pod kątem niskiej latencji. Wersja 3 z stycznia 2025 r. oferuje 81 % accuracy na MMLU i przepływ 150 tokenów/s; model został zaprojektowany tak, aby zmieścić się na pojedynczej karcie RTX 4090 lub MacBooku z 32 GB RAM Wersja 3.1 (marzec 2025) dodaje możliwość obsługi obrazów, funkcji i kontekstu 128k; nadal można ją uruchamiać na RTX 4090 lub MacBooku z 32 GB RAM  Wersja 3.2 (czerwiec 2025) to aktualizacja skupiająca się na lepszym przestrzeganiu instrukcji, redukcji powtórzeń i solidniejszym wywoływaniu funkcji – wewnętrzne miary wzrosły z 82,75 % do 84,78 % na testach instruction‑following zewnętrzne testy Wildbench i Arena Hard znacznie się poprawiły  3.2 wymaga ~55 GB VRAM w bf16, a dzięki kwantyzacji może działać na kartach z 32 GB RAM  Modele z serii Small są dostępne w licencji Apache 2.0 i stanowią fundament do fine‑tuningów, asystentów lokalnych, RAG oraz zadań multimodalnych.

### Magistral Small (24 B) i Magistral Medium
Magistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r. Blog Mistral AI opisuje ją jako dwa warianty: Magistral Small (24 B) udostępniony na licencji Apache 2.0 oraz Magistral Medium (wersja korporacyjna) Modele zostały zaprojektowane z myślą o transparentnym łańcuchu rozumowania (chain‑of‑thought), wielojęzyczności i obsłudze domen specjalistycznych.  Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %  Magistral Small ma kontekst 40k i minimalny wymóg 60 GB VRAM może być pobrany z HuggingFace i samodzielnie wdrożony Magistral Medium jest dostępny poprzez API i dla klientów chmurowych; Mistral podkreśla szybkość (Flash Answers w Le Chat) oraz zastosowania w prawie, finansach i regulacjach

### Devstral Small 1.1 i Devstral Medium
Devstral to rodzina modeli agentowych do pisania kodu. W lipcu 2025 r. udostępniono Devstral Small 1.1 pod licencją Apache 2.0 oraz Devstral Medium jako usługę API  Devstral Small 1.1 zachował architekturę 24 B, lecz osiągnął 53,6 % w benchmarku SWE‑Bench Verified oferując lepszą generalizację i obsługę formatu XML  Devstral Medium uzyskał 61,6 % na SWE‑Bench i jest dostępny do wdrożeń on‑premises; model można dostroić pod konkretne repozytoria  W ekosystemie Mistral Code Devstral współpracuje z embederami (Codestral Embed) i agentami (OpenHands).  Devstral Small zmieści się na pojedynczym RTX 4090 lub Macu z 32 GB RAM

### Voxtral Small 24 B i Voxtral Mini 3 B
Voxtral to pierwsza rodzina otwartych modeli audio firmy Mistral. Ogłoszona w lipcu 2025 r., składa się z wersji Small (24 B) i Mini (3 B). Oba modele są udostępnione na licencji Apache 2.0, a Voxtral Mini Transcribe (3 B) jest zoptymalizowany tylko do transkrypcji  Modele oferują kontekst 32k tokenów, co pozwala przetwarzać 30‑minutowe nagrania do transkrypcji lub 40‑minutowe nagrania w trybie rozumienia  Voxtral obsługuje wielojęzyczność, wbudowane Q&A i podsumowania audio, wywoływanie funkcji bezpośrednio z głosu oraz zachowuje możliwości tekstowe swojego trzonu (Mistral Small 3.1) Modele są gotowe do pobrania na HuggingFace; wymagają ok. 60 GB VRAM (Small) lub 8 GB (Mini)  Bench­marki publikowane w artykule pokazują, że Voxtral znacząco przewyższa Whisper large‑v3 i GPT‑4o mini w dokładności transkrypcji i rozumienia

### Mistral Saba
Mistral Saba to 24‑miliardowy model językowy zaprojektowany na potrzeby Bliskiego Wschodu i Azji Południowej. Artykuł z lutego 2025 r. opisuje go jako model szkolony na starannie dobranych danych arabskich i językach indyjskich  Saba obsługuje języki takie jak arabski i tamil, zapewniając bardziej naturalne odpowiedzi niż modele kilkakrotnie większe  Może działać lokalnie na pojedynczym GPU, oferując ponad 150 tokenów/s, i jest dostępny poprzez API oraz do samodzielnego wdrożenia w bezpiecznym środowisku  Zastosowania obejmują wirtualną obsługę klienta po arabsku, eksperckie doradztwo w branżach (energia, finanse, zdrowie) i tworzenie treści kulturowych  Wymagania sprzętowe są zbliżone do serii Small (ok. 60 GB VRAM).  

### Mistral Medium 3
Model Mistral Medium 3 (maj 2025) jest pierwszym „średnim” modelem w ofercie Mistral AI. Artykuł „Medium is the new large” podkreśla, że model zapewnia wydajność zbliżoną do modeli premium, przy ośmiokrotnie niższym koszcie i upro­sz­czonym wdrożeniu Medium 3 jest dostępny w formie API; można go także wdrożyć on‑premises na zestawie czterech GPU Model jest przeznaczony do zastosowań korporacyjnych: programowania, multimodalnej analizy i zadaniowych agentów; benchmarki pokazują, że osiąga ponad 90 % wyników Claude Sonnet 3.7 przy niższej cenie  Mistral Medium nie posiada publicznej tabeli wymagań VRAM; w praktyce wymaga wielu GPU.

### Magistral Medium 1.1
W lipcu 2025 r. firma ogłosiła aktualizację magistral Medium 1.1. Według changelogu, nowa wersja z datą 25.07 ma kontekst 40k i jest dostępna poprzez API  Ponieważ model jest premierowy, wagi nie są publiczne; licencja to Mistral Research/Commercial.

### Devstral Medium
Jak opisano w poście „Upgrading agentic coding capabilities with the new Devstral models”, Devstral Medium osiąga 61,6 % na SWE‑Bench Verified i jest dostępny poprzez API. Model umożliwia wdrożenia on‑premises oraz dostrojenie pod specyficzne repozytoria kodu  Podobnie jak Mistral Medium, jest to model premierowy, a zatem wagi i wymagania VRAM nie są publiczne.

### Codestral Embed
Codestral Embed to model embeddingowy wyspecjalizowany do kodu. Wersja 25.05 umożliwia generowanie wektorów o zmiennym wymiarze (np. 256 wymiarów w int8), przewyższając modele Voyage Code 3, Cohere Embed v4.0 i OpenAI Embeddings  Embed jest dostępny w API; oferuje elastyczną kontrolę kosztów, szybkie wyszukiwanie w repozytoriach, semantyczne wyszukiwanie, wykrywanie duplikatów i klastrowanie kodu  Zalecana długość kontekstu to 8 k tokenów, a do RAG zaleca się dzielenie dokumentów na fragmenty ~3000 znaków z nawarstwieniem 1000 znaków

### Mistral Embed
Mistral Embed to model embeddingowy dla tekstu (8 k kontekstu) dostępny jako usługa API. Został zaprojektowany do generowania gęstych reprezentacji semantycznych dla RAG i wyszukiwania. Chociaż Mistral nie opublikował dedykowanego bloga, dokumentacja API opisuje model jako dostarczający wektory 1024‑elementowe; usługa jest wykorzystywana w platformach takich jak Pinecone, Zilliz czy LangChain.

### Mistral OCR
Mistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu języków  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu języków  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźle  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.

### Ministral 3B i 8B
Seria Ministral to kompaktowe modele do zastosowań on‑device/edge. Zostały ogłoszone w październiku 2024 r. (3B i 8B) i udostępnione w licencji Mistral Commercial/Research. Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas. Używane są jako prywatne asystenty, do tłumaczenia offline, analityki na urządzeniach i robotyki  Minimalne wymagania VRAM to 8 GB dla 3B i 24 GB dla 8B  Wersje bazowe są dostępne na HuggingFace do użytku badawczego; licencja komercyjna wymagana jest do zastosowań produkcyjnych

## Modele wycofane
Zgodnie z dokumentacją, starsze modele Mistral (Mistral 7B, Mixtral 8×7B, Mixtral 8×22B oraz wersje Large 2407) zostały oznaczone jako przestarzałe i zostaną wycofane z platformy 30 marca 2025 r. lub 16 czerwca 2025 r. dla innych wersji Użytkownicy powinni migrować do nowszych modeli.

## Podsumowanie
Ekosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).

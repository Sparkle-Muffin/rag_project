
# Modele Mistral – kompendium (stan na sierpień 2025)

## Wprowadzenie
**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków.

## Podział na rodziny
- **Modele otwarte** – wagi są dostępne na HuggingFace i można je uruchamiać lokalnie. Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.). Wymagają znacznej pamięci VRAM podczas inferencji; tabela 1 podaje minimalne wartości.
- **Modele badawcze (Premier models)** – Mistral Large, Mixtral 8×22B, Pixtral Large, series Magistral Medium i Mistral Medium 3 mają wagi udostępnione tylko na zasadzie licencji badawczej (Mistral Research License) i są przeznaczone do testów niekomercyjnych. Samodzielne użycie wymaga licencji komercyjnej.
- **Modele komercyjne/API only** – Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych. Użytkownik wchodzi z nimi w relację usługową.

## Tabela 1 – skrócone parametry i wymagania VRAM
Poniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami.  Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly).

| Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |
|---|---|---|---|---|---|
| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |
| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |
| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |
| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |
| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |
| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |
| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |
| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |
| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |
| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |
| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |
| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |
| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |
| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |
| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |
| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |
| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |

## Opisy poszczególnych modeli
### Mistral 7B
Pierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space.  Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli.

### Mixtral 8×7B
Model typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22.5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu.

### Mixtral 8×22B
Kolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli.

### Codestral 22B (seria 25.01/25.08)
Codestral to open‑weight model przeznaczony do generowania kodu, wytrenowany na 80+ językach programowania z kontekstem 32k. Oferuje tryb *fill‑in‑the‑middle* (FIM) do uzupełniania brakujących fragmentów kodu i osiąga wysokie wyniki w HumanEval i MBPP.  Wersja 22 mld parametrów wymaga ok. 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060.  Licencja MNPL pozwala na badania i projekty niekomercyjnehttps://docs.mistral.ai/getting-started/models/weights/#:~:text=,are%20under%20Mistral%20Research%20License; do zastosowań komercyjnych konieczne jest wykupienie licencji.  W lipcu 2025 r. wprowadzono wersję 25.08, która zwiększyła odsetek akceptowanych uzupełnień kodu o 30 % i zmniejszyła liczbę „runaway generations” o 50 %https://mistral.ai/news/codestral-25-08#:~:text=Today%2C%20we%20announce%20its%20latest,measurable%20upgrades%20over%20prior%20versions.  Codestral jest dostępny w stacku Mistral Code oraz w usłudze Codestral Embed (embedding do wyszukiwania kodu).  

### Codestral Mamba 7B
Pierwszy model Mistral w architekturze Mamba2, specjalizowany w generowaniu kodu. Ma 7.285 mld parametrów i korzysta z liniowej złożoności obliczeniowej, co teoretycznie pozwala na nieskończony kontekst. Udostępniony w licencji Apache 2.0https://mistral.ai/news/codestral-mamba#:~:text=As%20a%20tribute%20to%20Cleopatra%2C,0%20license, sprawdza się jako lokalny asystent programistyczny o niskiej latencji. Minimalne wymagania VRAM są porównywalne z Mistral 7B (około 16 GB).  

### Mathstral 7B
Model o 7 mld parametrach z kontekstem 32k przeznaczony do zadań matematycznych i STEM. Współpraca z projektem Numina zaowocowała wysokimi wynikami: 56,6 % na zbiorze MATH i 63,47 % na MMLUhttps://mistral.ai/news/mathstral#:~:text=As%20a%20tribute%20to%20Archimedes%2C,0%20license. Model, dostępny na licencji Apache 2.0, zachowuje zgodność z architekturą Mistral 7B, dzięki czemu może pełnić rolę matematycznego eksperta w RAG.

### Mistral NeMo 12B
Model 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages.  Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.

### Mistral Large 2 i Mistral Large 24.11
Mistral Large 2, ogłoszony w lipcu 2024 r., to flagowy model Mistrala z 123 mld parametrów i kontekstem 128k. Blog „Large Enough” podkreśla, że model znacząco poprawia generowanie kodu, matematyczne rozumowanie i obsługę wielu językówhttps://mistral.ai/news/mistral-medium-3#:~:text=Medium%20is%20the%20new%20large. Model wydany jest w licencji badawczej; w pełnej precyzji wymaga ok. 233 GB VRAM, a kwantyzacja int4 redukuje wymagania do ~58 GBhttps://huggingface.co/mistralai/Mistral-Large-Instruct-2407/discussions/15#:~:text=You%20will%20need%20about%20,to%20train%20it%20using%20Adam.  Wersja 24.11 (Mistral Large 2.1) z listopada 2024 r. wprowadziła nowe podpowiedzi systemowe, lepsze wywoływanie funkcji i ulepszone wnioskowanie długokontekstowehttps://mistral.ai/news/pixtral-large#:~:text=Along%20with%20Pixtral%20Large%2C%20Mistral,Mistral%20AI%20for%20commercial%20use.

### Pixtral 12B i Pixtral Large
Pixtral 12B to pierwszy multimodalny model Mistrala. Składa się z 12 mld parametrów w dekoderze (na bazie Mistral NeMo) i 400 mln parametrów w enkoderze obrazu; model przyjmuje kilka obrazów oraz tekst, obsługuje zmienne rozmiary obrazów i ma 128k kontekstuhttps://mistral.ai/news/pixtral-12b#:~:text=Pixtral%2012B%20in%20short%3Ahttps://mistral.ai/news/pixtral-12b#:~:text=Pixtral%20was%20trained%20to%20be,instruction%20following%2C%20coding%2C%20and%20math. Jest dostępny w licencji Apache 2.0; w precyzji bf16 wymaga 28 GB VRAM, w fp8 około 16 GBhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8; artykuł Ori podaje, że do uruchomienia na vLLM wystarczy 24 GB VRAMhttps://www.ori.co/blog/how-to-run-pixtral-on-a-cloud-gpu#:~:text=memory%20for%20this%20demo%20because,Debian%20is%20also%20an%20option. Pixtral Large (124 mld parametrów + 1 mld w enkoderze obrazu) ma kontekst 128k i przewyższa konkurencję na benchmarkach MathVista, DocVQA i VQA v2https://mistral.ai/news/pixtral-large#:~:text=Pixtral%20Large%20in%20short%3A. Udostępniony jest w licencji badawczej; wymaga ok. 250 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,124B%20124B%20250.

### Mistral Small 3, 3.1 i 3.2 (24 B parametrów)
Seria Mistral Small to 24‑miliardowe modele zoptymalizowane pod kątem niskiej latencji. Wersja 3 z stycznia 2025 r. oferuje 81 % accuracy na MMLU i przepływ 150 tokenów/s; model został zaprojektowany tak, aby zmieścić się na pojedynczej karcie RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3#:~:text=Mistral%20Small%203%20is%20a,performance%2C%20with%20very%20low%20latencyhttps://mistral.ai/news/mistral-small-3#:~:text=,a%20Macbook%20with%2032GB%20RAM. Wersja 3.1 (marzec 2025) dodaje możliwość obsługi obrazów, funkcji i kontekstu 128k; nadal można ją uruchamiać na RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3-1#:~:text=performance%2C%20multimodal%20understanding%2C%20and%20an,of%20150%20tokens%20per%20second.  Wersja 3.2 (czerwiec 2025) to aktualizacja skupiająca się na lepszym przestrzeganiu instrukcji, redukcji powtórzeń i solidniejszym wywoływaniu funkcji – wewnętrzne miary wzrosły z 82,75 % do 84,78 % na testach instruction‑followinghttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks; zewnętrzne testy Wildbench i Arena Hard znacznie się poprawiłyhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks.  3.2 wymaga ~55 GB VRAM w bf16, a dzięki kwantyzacji może działać na kartach z 32 GB RAMhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=Mistral%20Small%203,serve%20access%20and%20direct%20deployment.  Modele z serii Small są dostępne w licencji Apache 2.0 i stanowią fundament do fine‑tuningów, asystentów lokalnych, RAG oraz zadań multimodalnych.

### Magistral Small (24 B) i Magistral Medium
Magistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r. Blog Mistral AI opisuje ją jako dwa warianty: Magistral Small (24 B) udostępniony na licencji Apache 2.0 oraz Magistral Medium (wersja korporacyjna)https://mistral.ai/news/magistral#:~:text=Today%2C%20we%E2%80%99re%20excited%20to%20announce,along%20with%20deep%20multilingual%20flexibility. Modele zostały zaprojektowane z myślą o transparentnym łańcuchu rozumowania (chain‑of‑thought), wielojęzyczności i obsłudze domen specjalistycznych.  Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %https://mistral.ai/news/magistral#:~:text=,a%20more%20powerful%2C%20enterprise%20version.  Magistral Small ma kontekst 40k i minimalny wymóg 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,2507%2024B%2024B%2060; może być pobrany z HuggingFace i samodzielnie wdrożonyhttps://mistral.ai/news/magistral#:~:text=Availability. Magistral Medium jest dostępny poprzez API i dla klientów chmurowych; Mistral podkreśla szybkość (Flash Answers w Le Chat) oraz zastosowania w prawie, finansach i regulacjachhttps://mistral.ai/news/magistral#:~:text=With%20Flash%20Answers%20in%20Le,and%20user%20feedback%2C%20at%20scale.

### Devstral Small 1.1 i Devstral Medium
Devstral to rodzina modeli agentowych do pisania kodu. W lipcu 2025 r. udostępniono Devstral Small 1.1 pod licencją Apache 2.0 oraz Devstral Medium jako usługę APIhttps://mistral.ai/news/devstral-2507#:~:text=Today%2C%20we%20introduce%20Devstral%20Medium%2C,different%20prompts%20and%20agentic%20scaffolds.  Devstral Small 1.1 zachował architekturę 24 B, lecz osiągnął 53,6 % w benchmarku SWE‑Bench Verifiedhttps://mistral.ai/news/devstral-2507#:~:text=Enhanced%20Performance, oferując lepszą generalizację i obsługę formatu XMLhttps://mistral.ai/news/devstral-2507#:~:text=Devstral%20Small%201,of%20applications%20and%20agentic%20scaffolds.  Devstral Medium uzyskał 61,6 % na SWE‑Bench i jest dostępny do wdrożeń on‑premises; model można dostroić pod konkretne repozytoriahttps://mistral.ai/news/devstral-2507#:~:text=Devstral%20Medium%20builds%20upon%20the,effective%20model.  W ekosystemie Mistral Code Devstral współpracuje z embederami (Codestral Embed) i agentami (OpenHands).  Devstral Small zmieści się na pojedynczym RTX 4090 lub Macu z 32 GB RAMhttps://mistral.ai/news/codestral-25-08#:~:text=,open%20models%20by%20wide%20margins.

### Voxtral Small 24 B i Voxtral Mini 3 B
Voxtral to pierwsza rodzina otwartych modeli audio firmy Mistral. Ogłoszona w lipcu 2025 r., składa się z wersji Small (24 B) i Mini (3 B). Oba modele są udostępnione na licencji Apache 2.0, a Voxtral Mini Transcribe (3 B) jest zoptymalizowany tylko do transkrypcjihttps://mistral.ai/news/voxtral#:~:text=We%20release%20the%20Voxtral%20models,behind%20Voxtral%2C%20please%20refer%20to.  Modele oferują kontekst 32k tokenów, co pozwala przetwarzać 30‑minutowe nagrania do transkrypcji lub 40‑minutowe nagrania w trybie rozumieniahttps://mistral.ai/news/voxtral#:~:text=include%3A.  Voxtral obsługuje wielojęzyczność, wbudowane Q&A i podsumowania audio, wywoływanie funkcji bezpośrednio z głosu oraz zachowuje możliwości tekstowe swojego trzonu (Mistral Small 3.1)https://mistral.ai/news/voxtral#:~:text=%2A%20Long,or%2040%20minutes%20for%20understanding. Modele są gotowe do pobrania na HuggingFace; wymagają ok. 60 GB VRAM (Small) lub 8 GB (Mini)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Devstral,Mini%203B%203B%208.  Bench­marki publikowane w artykule pokazują, że Voxtral znacząco przewyższa Whisper large‑v3 i GPT‑4o mini w dokładności transkrypcji i rozumieniahttps://mistral.ai/news/voxtral#:~:text=Voxtral%20comprehensively%20outperforms%20Whisper%20large,demonstrating%20its%20strong%20multilingual%20capabilities.

### Mistral Saba
Mistral Saba to 24‑miliardowy model językowy zaprojektowany na potrzeby Bliskiego Wschodu i Azji Południowej. Artykuł z lutego 2025 r. opisuje go jako model szkolony na starannie dobranych danych arabskich i językach indyjskichhttps://mistral.ai/news/mistral-saba#:~:text=Mistral%20Saba%20is%20a%2024B,serve%20as%20a%20strong%20base.  Saba obsługuje języki takie jak arabski i tamil, zapewniając bardziej naturalne odpowiedzi niż modele kilkakrotnie większehttps://mistral.ai/news/mistral-saba#:~:text=from%20across%20the%20Middle%20East,over%20150%20tokens%20per%20second.  Może działać lokalnie na pojedynczym GPU, oferując ponad 150 tokenów/s, i jest dostępny poprzez API oraz do samodzielnego wdrożenia w bezpiecznym środowiskuhttps://mistral.ai/news/mistral-saba#:~:text=to%20train%20highly%20specific%20regional,over%20150%20tokens%20per%20second.  Zastosowania obejmują wirtualną obsługę klienta po arabsku, eksperckie doradztwo w branżach (energia, finanse, zdrowie) i tworzenie treści kulturowychhttps://mistral.ai/news/mistral-saba#:~:text=As%20Mistral%20Saba%20gains%20traction,emerging%2C%20showcasing%20the%20model%27s%20versatility.  Wymagania sprzętowe są zbliżone do serii Small (ok. 60 GB VRAM).  

### Mistral Medium 3
Model Mistral Medium 3 (maj 2025) jest pierwszym „średnim” modelem w ofercie Mistral AI. Artykuł „Medium is the new large” podkreśla, że model zapewnia wydajność zbliżoną do modeli premium, przy ośmiokrotnie niższym koszcie i upro­sz­czonym wdrożeniuhttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20state,with%20radically%20simplified%20enterprise%20deployments. Medium 3 jest dostępny w formie API; można go także wdrożyć on‑premises na zestawie czterech GPUhttps://mistral.ai/news/mistral-medium-3#:~:text=Additionally%2C%20Mistral%20Medium%203%20can,of%20four%20GPUs%20and%20above. Model jest przeznaczony do zastosowań korporacyjnych: programowania, multimodalnej analizy i zadaniowych agentów; benchmarki pokazują, że osiąga ponad 90 % wyników Claude Sonnet 3.7 przy niższej ceniehttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20frontier,2%20output%20per%20M%20token.  Mistral Medium nie posiada publicznej tabeli wymagań VRAM; w praktyce wymaga wielu GPU.

### Magistral Medium 1.1
W lipcu 2025 r. firma ogłosiła aktualizację magistral Medium 1.1. Według changelogu, nowa wersja z datą 25.07 ma kontekst 40k i jest dostępna poprzez APIhttps://docs.mistral.ai/getting-started/models/models_overview/#:~:text=Premier%20models.  Ponieważ model jest premierowy, wagi nie są publiczne; licencja to Mistral Research/Commercial.

### Devstral Medium
Jak opisano w poście „Upgrading agentic coding capabilities with the new Devstral models”, Devstral Medium osiąga 61,6 % na SWE‑Bench Verified i jest dostępny poprzez API. Model umożliwia wdrożenia on‑premises oraz dostrojenie pod specyficzne repozytoria koduhttps://mistral.ai/news/devstral-2507#:~:text=Devstral%20Medium%20builds%20upon%20the,effective%20model.  Podobnie jak Mistral Medium, jest to model premierowy, a zatem wagi i wymagania VRAM nie są publiczne.

### Codestral Embed
Codestral Embed to model embeddingowy wyspecjalizowany do kodu. Wersja 25.05 umożliwia generowanie wektorów o zmiennym wymiarze (np. 256 wymiarów w int8), przewyższając modele Voyage Code 3, Cohere Embed v4.0 i OpenAI Embeddingshttps://mistral.ai/news/codestral-embed#:~:text=We%20are%20excited%20to%20release,world%20code%20data.  Embed jest dostępny w API; oferuje elastyczną kontrolę kosztów, szybkie wyszukiwanie w repozytoriach, semantyczne wyszukiwanie, wykrywanie duplikatów i klastrowanie koduhttps://mistral.ai/news/codestral-embed#:~:text=Use%20cases.  Zalecana długość kontekstu to 8 k tokenów, a do RAG zaleca się dzielenie dokumentów na fragmenty ~3000 znaków z nawarstwieniem 1000 znakówhttps://mistral.ai/news/codestral-embed#:~:text=For%20retrieval%20use%20cases%2C%20while,for%20more%20information%20about%20chunking.

### Mistral Embed
Mistral Embed to model embeddingowy dla tekstu (8 k kontekstu) dostępny jako usługa API. Został zaprojektowany do generowania gęstych reprezentacji semantycznych dla RAG i wyszukiwania. Chociaż Mistral nie opublikował dedykowanego bloga, dokumentacja API opisuje model jako dostarczający wektory 1024‑elementowe; usługa jest wykorzystywana w platformach takich jak Pinecone, Zilliz czy LangChain.

### Mistral OCR
Mistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images.  Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top.  Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category.  Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag.

### Ministral 3B i 8B
Seria Ministral to kompaktowe modele do zastosowań on‑device/edge. Zostały ogłoszone w październiku 2024 r. (3B i 8B) i udostępnione w licencji Mistral Commercial/Research. Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas. Używane są jako prywatne asystenty, do tłumaczenia offline, analityki na urządzeniach i robotykihttps://mistral.ai/news/ministraux#:~:text=Introducing%20the%20world%E2%80%99s%20best%20edge,models.  Minimalne wymagania VRAM to 8 GB dla 3B i 24 GB dla 8Bhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Voxtral,3B%203B%208.  Wersje bazowe są dostępne na HuggingFace do użytku badawczego; licencja komercyjna wymagana jest do zastosowań produkcyjnychhttps://mistral.ai/news/ministraux#:~:text=Ministral%208B%20ministral,Mistral%20Commercial%20License.

## Modele wycofane
Zgodnie z dokumentacją, starsze modele Mistral (Mistral 7B, Mixtral 8×7B, Mixtral 8×22B oraz wersje Large 2407) zostały oznaczone jako przestarzałe i zostaną wycofane z platformy 30 marca 2025 r. lub 16 czerwca 2025 r. dla innych wersjihttps://docs.mistral.ai/getting-started/models/models_overview/#:~:text=Legacy%20modelshttps://docs.mistral.ai/getting-started/models/models_overview/#:~:text=Mistral%20Medium%202312%20%60mistral,latest%20%60%20Mistral%20Large%202407%E2%9C%94%EF%B8%8F. Użytkownicy powinni migrować do nowszych modeli.

## Podsumowanie
Ekosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością.  Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute).

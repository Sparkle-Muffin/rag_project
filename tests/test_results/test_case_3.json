{
    "question": "Jaki model najlepiej działa na GPU z 24 GB VRAM?",
    "required_keywords": [
        "13b",
        "24 gb",
        "gpu"
    ],
    "optional_keywords": [
        "llama 2 13b",
        "llama 3 8b",
        "llama 3.1 8b",
        "llama 3.2 3b",
        "mistral 7b",
        "mistral nemo 12b",
        "pixtral 12b",
        "pllum 12b",
        "gpt oss 20b",
        "llama 3.2 vision 11b"
    ],
    "expected_answer": "Najlepiej działa LLaMa-13B, zoptymalizowany do takiej konfiguracji.",
    "model_answer": "Na podstawie dostarczonego kontekstu, modele LLama i Mistral mają różne wymagania sprzętowe. Dla GPU z 24 GB VRAM, odpowiednie mogą być następujące modele:\n\n1. **LLama**:\n   - **LLama 3 (70B)**: W pełnej precyzji wymaga ponad 140 GB VRAM, ale z technikami kwantyzacji (np. 4-bit) może działać na GPU RTX 4090 (24 GB).\n   - **LLama 1 (7B lub 13B)**: Wersje kwantyzowane (np. 8-bit lub 4-bit) mogą działać na kartach z 6–13 GB VRAM, ale w pełnej precyzji wymagają więcej pamięci.\n\n2. **Mistral**:\n   - **Mistral Small (24B)**: Wymaga około 60 GB VRAM w pełnej precyzji, co przekracza dostępne 24 GB. Jednak kwantyzowane wersje mogą być bardziej przyjazne dla sprzętu.\n   - **Mistral 7B**: Wersja 4-bitowa może działać nawet na kartach z 4 GB VRAM, a wersja 16-bit (BF16) wymaga około 16 GB. Dla 24 GB VRAM, wersja 16-bitowa powinna działać bez problemu.\n\n3. **PLLUM**:\n   - Mniejsze warianty (8B lub 12B) są bardziej przyjazne dla GPU klasy konsumenckiej i mogą działać na 24 GB VRAM.\n\n**Podsumowanie**: Najlepiej sprawdzi się **Mistral 7B w wersji 16-bit (BF16)**, który wymaga około 16 GB VRAM, lub mniejsze modele LLama (np. 7B z kwantyzacją). Dla większych modeli (np. Mistral Small), konieczna będzie kwantyzacja lub użycie wielu GPU.",
    "required_keywords_score": 100.0,
    "optional_keywords_score": 10.0,
    "total_keywords_score": 30.76923076923077,
    "evaluation_score": 7,
    "descriptive_evaluation": "Odpowiedź modelu jest częściowo poprawna i zawiera szczegółowe informacje o wymaganiach VRAM dla różnych modeli. Wskazuje na kilka opcji, które mogą działać na GPU z 24 GB VRAM (np. Mistral 7B 16-bit lub mniejsze modele LLama). Jednak nie podaje jednoznacznej odpowiedzi, która model najlepiej się sprawdzi w tej konfiguracji, co jest kluczowe dla pytania. Zamiast tego przedstawia kilka możliwości, co czyni ją nieco mniej precyzyjną niż odpowiedź wzorcowa.",
    "answer_generation_time_s": 87
}
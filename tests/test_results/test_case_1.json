{
    "question": "Jakie modele LLaMa są dostępne?",
    "required_keywords": [
        "llama 1",
        "llama 2",
        "llama 3",
        "llama 4"
    ],
    "optional_keywords": [
        "llama 3.1",
        "llama 3.2",
        "llama 4 scout",
        "llama 4 maverick",
        "llama 4 behemoth",
        "code llama",
        "llama 3.1 8b",
        "llama 3.1 70b",
        "llama 3.1 405b",
        "llama 3.2 1b",
        "llama 3.2 3b",
        "llama 3.2 vision 11b",
        "llama 3.2 vision 90b",
        "llama 3 8b",
        "llama 3 70b",
        "llama 2 7b",
        "llama 2 13b",
        "llama 2 70b",
        "llama 1 7b",
        "llama 1 13b",
        "llama 1 65b"
    ],
    "expected_answer": "Dostępne są modele LLaMa 1, LLaMa 2, LLaMa 3 i LLaMa 4.",
    "model_answer": "Według podanego kontekstu, rodzina modeli LLaMA obejmuje następujące modele (stan na sierpień 2025 r.):\n\n1. **LLaMA 1** (2023):\n   - Warianty: 7B, 13B, 65B parametrów.\n   - Obsługują kontekst około 2K tokenów.\n   - Dostępne wyłącznie na zasadach badawczych.\n\n2. **LLaMA 2** (lipiec 2023):\n   - Warianty: 7B, 13B, 70B.\n   - Trenowane na większym zbiorze danych.\n   - Licencjonowane na zasadach LLaMA Community License.\n\n3. **CodeLLama** (sierpień 2023):\n   - Warianty: 7B, 13B, 30B/33B/34B.\n   - Specjalizują się w generacji kodu i obsłudze zadań programistycznych.\n   - Działają z długim kontekstem (do 100K tokenów).\n\n4. **LLaMA 3.2** (wrzesień 2024):\n   - Warianty: 1B, 3B (tekstowe) oraz multimodalne warianty LLaMA 3.2-vision (11B i 90B).\n   - Modele 1B i 3B obsługują okna kontekstowe do 128K tokenów.\n   - Trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski).\n\n5. **LLaMA 4** (kwiecień 2025):\n   - Warianty: Scout i Maverick.\n   - Przeznaczone do tworzenia asystentów multimodalnych, zaawansowanych aplikacji RAG z ogromnym kontekstem (kilka milionów tokenów) i systemów budujących agentów z długą pamięcią.\n\nDodatkowo, modele Mistral są powiązane z LLaMA i obejmują różne warianty (np. Mistral 7B, MixTral, CodestrAl Mamba, MathstrAl, PixtrAl 12B), które są dostępne na Hugging Face i podlegają licencji Apache 2.0.\n\nWymagania sprzętowe różnią się w zależności od modelu: mniejsze modele (1B–8B) mogą działać na pojedynczej karcie graficznej z 16GB VRAM lub nawet na CPU z odpowiednią ilością RAM, podczas gdy większe modele (70B i więcej) wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci.",
    "required_keywords_score": 100.0,
    "optional_keywords_score": 4.761904761904762,
    "total_keywords_score": 20.0,
    "evaluation_score": 9,
    "descriptive_evaluation": "Odpowiedź modelu jest bardzo szczegółowa i zawiera więcej informacji niż odpowiedź wzorcowa. Obejmuje ona nie tylko nazwy modeli, ale także ich warianty parametrów, daty wydania, specjalizację oraz wymagania sprzętowe. Jednakże, odpowiedź wykracza poza zakres pytania, które dotyczyło jedynie dostępnych modeli LLaMa.",
    "answer_generation_time_s": 114
}
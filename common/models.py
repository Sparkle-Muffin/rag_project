"""
Pydantic models for the RAG project data structures.

This module defines the core data models used throughout the RAG system,
providing type safety, validation, and consistent data structures.
"""

from pydantic import BaseModel, Field
from typing import List, Optional, Union, Dict, Any
from pathlib import Path


class EmbeddingMetadata(BaseModel):
    """Model for embedding metadata and vectors."""
    
    text: str = Field(..., description="Text content of the chunk")
    id: int = Field(..., description="Unique identifier for the chunk")
    vector: List[float] = Field(..., description="Embedding vector")


class SearchResult(BaseModel):
    """Model for search results from both vector and BM25 search."""
    
    id: int = Field(..., description="Document/chunk identifier")
    score: float = Field(..., description="Relevance score")
    text: str = Field(..., description="Text content of the result")


class HybridSearchResult(BaseModel):
    """Model for combined search results with scores from both methods."""
    
    id: int = Field(..., description="Document/chunk identifier")
    text: str = Field(..., description="Text content of the result")
    qdrant_score: Optional[float] = Field(None, description="Score from vector search")
    bm25_score: Optional[float] = Field(None, description="Score from BM25 search")
    combined_score: float = Field(..., description="Combined reciprocal rank fusion score")


class TestCase(BaseModel):
    """Model for test case data."""
    
    question: str = Field(..., description="Question to test")
    required_keywords: List[str] = Field(..., description="Required keywords that must be present")
    optional_keywords: Optional[List[str]] = Field(None, description="Optional keywords")
    expected_answer: str = Field(..., description="Expected/ground truth answer")


class KeywordScores(BaseModel):
    """Model for keyword coverage test results."""
    
    required_score: Optional[float] = Field(None, description="Percentage of required keywords covered")
    optional_score: Optional[float] = Field(None, description="Percentage of optional keywords covered")
    total_score: Optional[float] = Field(None, description="Overall keyword coverage percentage")


class TestResult(BaseModel):
    """Model for complete test results."""
    
    question: str = Field(..., description="Question that was tested")
    required_keywords: List[str] = Field(..., description="Required keywords for the test")
    optional_keywords: Optional[List[str]] = Field(None, description="Optional keywords for the test")
    expected_answer: str = Field(..., description="Expected answer")
    model_answer: str = Field(..., description="Answer generated by the model")
    required_keywords_score: Optional[float] = Field(None, description="Required keywords coverage score")
    optional_keywords_score: Optional[float] = Field(None, description="Optional keywords coverage score")
    total_keywords_score: Optional[float] = Field(None, description="Total keywords coverage score")
    evaluation_score: int = Field(..., ge=1, le=10, description="LLM-as-a-judge evaluation score (1-10)")
    descriptive_evaluation: str = Field(..., description="Descriptive evaluation from LLM-as-a-judge")
    answer_generation_time_s: int = Field(..., ge=0, description="Time taken to generate answer in seconds")


class ModelResponse(BaseModel):
    """Model for model API responses."""
    
    response: str = Field(..., description="Model's response text")
    done: bool = Field(False, description="Whether the response is complete")
    error: Optional[str] = Field(None, description="Error message if any")


class PromptData(BaseModel):
    """Model for prompt generation data."""
    
    system_prompt: str = Field(..., description="System prompt for the model")
    user_prompt: str = Field(..., description="User's question or input")
    db_chunks_number: int = Field(..., ge=1, le=100, description="Number of chunks to retrieve from database")
    model_context_chunks_number: int = Field(..., ge=1, le=50, description="Number of chunks to pass to model")


class SearchQuery(BaseModel):
    """Model for search queries."""
    
    query: str = Field(..., description="Search query text")
    collection_name: str = Field(..., description="Name of the collection to search in")
    max_results: int = Field(..., ge=1, description="Maximum number of results to return")


class FileProcessingConfig(BaseModel):
    """Model for file processing configuration."""
    
    input_dir: Path = Field(..., description="Input directory path")
    output_dir: Path = Field(..., description="Output directory path")
    chunk_size: Optional[int] = Field(None, description="Size of text chunks")
    overlap: Optional[int] = Field(0, description="Overlap between chunks")


class BM25Config(BaseModel):
    """Model for BM25 encoding configuration."""
    
    input_dir: Path = Field(..., description="Directory containing text files to encode")
    encodings_db_path: str = Field(..., description="Path to save BM25 model and corpus")
    stopwords: str = Field("en", description="Language for stopwords removal")


class QdrantConfig(BaseModel):
    """Model for Qdrant database configuration."""
    
    collection_name: str = Field(..., description="Name of the collection")
    vector_size: int = Field(..., gt=0, description="Dimension of embedding vectors")
    distance_metric: str = Field("COSINE", description="Distance metric for similarity search")
    url: str = Field("http://localhost:6333", description="Qdrant server URL")
    port: int = Field(6333, description="Qdrant server port") 
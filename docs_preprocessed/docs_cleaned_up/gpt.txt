# modele gpt – szczegółowy przegląd dla rag (stan na sierpień 2025)

generative pre‑trained transformer (gpt) to seria modeli językowych firmy openai.  każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań.  poniższy plik zawiera usystematyzowaną wiedzę na temat modeli gpt od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach.  modele gpt‑3/3.5/4 dostępne są tylko poprzez api; jedynymi otwartymi wagami od openai są nowo zaprezentowane modele gpt‑oss.

## 1. ewolucja serii gpt

 generacja  liczba parametrów  okno kontekstowe i wykorzystanie  wymagania pamięci

a (przybliżone do ład. inference)  licencja i dostępność  cechy i zastosowania 

------------------

 gpt‑1 (czerwiec 2018)  ~117 mln parametrów  model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie bookscorpus; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjami  wagi mają ~0,5 gb; do uruchomienia w fp16 wystarcza konsumencki gpu.  wydany na licencji mit; wagi udostępniono publicznie poprzez github.  pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię gpt. 

 gpt‑2 (luty 2019)  1,5 mld parametrów (wersja pełna); udostępniono także warianty 124 m, 355 m i 774 m  używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż gpt‑1). pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitach  wagi modelu pełnego mają ok. 5 gb i pochłaniają dużo ram podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć cpu w 100%  dla fp32 potrzeba ~6 gb vram, co potwierdzają przewodniki dotyczące uruchamiania gpt‑2  licencja mit; openai w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje.  model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. 

 gpt‑3 (czerwiec 2020)  175 mld parametrów  standardowe okno kontekstowe ma 2 048 tokenów model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu.  składa się w 60 % z danych common crawl, 22 % z książek i treści licencjonowanych oraz 3 % z wikipedii  dla fp16/bfloat16 potrzeba ~350 gb pamięci gpu (2 bajty × 175 mld parametrów) w praktyce model jest dostępny tylko w chmurze.  model komercyjny – dostępny wyłącznie poprzez api openai/microsoft; prawa do wag posiada microsoft.  umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania chatgpt. 

 gpt‑3.5 / chatgpt (2022)  ok. 175 mld parametrów, model „instructgpt” fine‑tuningowany z użyciem rlhf.  pierwsza wersja chatgpt miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono gpt‑3.5 turbo z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenów  vram jest podobny jak dla gpt‑3 (kilkaset gb w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele gpu.  model własnościowy; udostępniany poprzez usługę chatgpt (free/plus) i api, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli).  chatgpt wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. 

 gpt‑4 (marzec 2023)  dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)  wersje mają okna 8 192 i 32 768 tokenów w listopadzie 2023 wprowadzono gpt‑4 turbo, które oferuje okno 128 k i niższą cenę  ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów vram; szczegóły sprzętowe nie zostały upublicznione.  licencja komercyjna; dostępny wyłącznie w api i płatnych subskrypcjach chatgpt plus/enterprise.  model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficzne  przewyższa gpt‑3.5 w rozumieniu poleceń i kodowaniu. 

 gpt‑4o ("omni") (maj 2024)  stanowi rozszerzenie gpt‑4 z niewielką liczbą parametrów (szczegóły niejawne).  zachowuje 128 k kontekstu, ale dodaje multimodalny input/ output: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistym  wymaga mniejszej mocy niż pełne gpt‑4, jednak szczegóły pamięci są tajne; działa poprzez api.  komercyjny, dostępny w chatgpt plus i wybranych api.  pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniach 

 gpt‑4.5 (wczesny 2025)  według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; openai opisuje go jako „gigantyczny, kosztowny model”  ma podobne lub większe okno kontekstowe niż gpt‑4 turbo (≥128 k).  wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez api.  komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source.  ulepszona wersja gpt‑4 z większą bazą wiedzy i większym „eq”; generuje bardziej naturalne i pogłębione odpowiedzi 

 gpt‑4.1 (kwiecień 2025)  ulepszona wersja gpt‑4o skoncentrowana na kodowaniu i długich kontekstach   obsługuje kontekst do 1 mln tokenów – dziesięciokrotnie więcej niż gpt‑4 turbo  wprowadza sub‑modele mini i nano oraz obsługę „tools” w api.  ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne.  dostępny tylko poprzez api; licencja komercyjna.  zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentów 

 o‑series (o3, o4‑mini, o4 etc.) (2024‑2025)  modele dedykowane do reasoningu i agentowego korzystania z narzędzi – o3 (2024) i o4‑mini (kwiecień 2025) są mniejsze niż standardowe gpt‑4.  o‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzi  o4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznych  modele te wymagają mniejszych zasobów niż pełne gpt‑4; o4‑mini ma czas reakcji zbliżony do chatgpt, lecz nadal potrzebuje gpu.  komercyjne; dostępne w chatgpt i wybranych api.  o‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowych 

 gpt‑oss (sierpień 2025) – open‑weights  gpt‑oss‑120b: 117 mld parametrów, 5,1 mld aktywnych parametrów; gpt‑oss‑20b: 21 mld parametrów, 3,6 mld aktywnych  modele korzystają z kwantyzacji mxfp4 i obsługują 128 k tokenów dzięki pozycjonowaniu rope; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowego  gpt‑oss‑120b zmieści się na jednej karcie h100 80 gb, a gpt‑oss‑20b można uruchomić na gpu z 16 gb vram  pliki wag wymagają 80 gb (120b) lub 16 gb (20b) pamięci vram dzięki 4‑bitowej kwantyzacji  licencja apache 2.0 z niewielkim uzupełnieniem dotyczącym bezpiecznego użycia  modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędzia  dzięki otwartym wagom umożliwiają lokalny rag, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vllm czy llama.cpp. 

## 2. kluczowe wnioski i zalecenia dla rag

1. parametry i kontekst – w serii gpt liczba parametrów wzrosła z 117 mln (gpt‑1) do ok. 1 biliona (gpt‑4), a kontekst z 512 do nawet 1 mln tokenów (gpt‑4.1).  dłuższe okna kontekstowe są szczególnie przydatne w systemach rag, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.

2. wymagania sprzętowe – komercyjne modele gpt‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci gpu do załadowania wag.  wersja gpt‑oss‑120b mieści się na pojedynczym h100 (80 gb), a gpt‑oss‑20b działa nawet na karcie konsumenckiej z 16 gb vram  jeżeli planujesz lokalny rag, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ gpt‑3/4 wymagają klastrów gpu.

3. licencje i wykorzystanie – wczesne modele gpt‑1 i gpt‑2 opublikowano na licencji mit; jednak od gpt‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością openai i udostępniane są tylko przez api.  oznacza to, że do budowy własnego systemu rag musisz używać modeli open‑source (np. gpt‑oss, llama czy mistral) lub korzystać z api w chmurze.  modele gpt‑oss dostępne są na licencji apache 2.0, co umożliwia komercyjne wykorzystanie i dalsze modyfikacje

4. zastosowania – gpt‑3 umożliwił generowanie esejów, programów i dialogów, natomiast gpt‑3.5 wprowadził rlhf i lepszą konwersację (chatgpt).  gpt‑4 rozszerzył możliwości na multimodalność (obrazy), a gpt‑4o dodał wejście i wyjście audio oraz skrócił czas reakcji  gpt‑4.1 oraz o‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter python, generowanie obrazów)  dla lokalnych systemów rag zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.

5. benchmarki i wyniki – openai nie ujawnia pełnych benchmarków dla gpt‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. sat, lsat)  o‑seria (o3, o4‑mini) ustanowiła rekord na konkursie aime 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzi

## 3. podsumowanie

seria gpt ewoluowała od niewielkiego modelu z 2018 r. do ogromnych systemów multimodalnych, które mogą przetwarzać milion tokenów i rozwiązywać złożone zadania.  w kontekście rag kluczowe jest zrozumienie, że modele gpt > 3 są własnościowe i wymagają ogromnych zasobów; dlatego w praktycznych wdrożeniach, zwłaszcza on‑premise, lepiej korzystać z otwartych modeli takich jak gpt‑oss, llama, mistral lub innych llm open‑source.  jeśli planujesz użyć gpt‑3/4 w rag, pamiętaj o limitach tokenów (4k–128k) oraz kosztach api; natomiast gpt‑oss zapewnia 128 k kontekstu i licencję apache 2.0, co czyni go atrakcyjnym wyborem do długich dokumentów i rozbudowanych pipeline’ów.
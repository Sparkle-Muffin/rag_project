# ğŸ“š RAG QA Chat z Bielikiem i Qdrant

## ğŸ¯ Cel projektu
Celem projektu byÅ‚o stworzenie kompletnego systemu **RAG (Retrieval-Augmented Generation)**.
System pozwala uÅ¼ytkownikowi zadawaÄ‡ pytania dotyczÄ…ce treÅ›ci dokumentÃ³w, ktÃ³re wczeÅ›niej zostaÅ‚y:

- oczyszczone i ujednolicone,
- podzielone na **chunki**,
- zamienione na **embeddingi** (model *mmlw-roberta-large*) i zapisane w bazie wektorowej **Qdrant**,
- zamienione na encodingi MB25 i zapisane w bazie encodingÃ³w.

Na tej podstawie system wyszukuje najbardziej adekwatne fragmenty dokumentÃ³w i przekazuje je do duÅ¼ego modelu jÄ™zykowego **Bielik-11B-v2.6-Instruct**, ktÃ³ry generuje odpowiedÅº prezentowanÄ… uÅ¼ytkownikowi w formie czatu.

---

## ğŸš€ Jak uruchomiÄ‡ projekt

### ğŸ†• Szybki start (zalecane)
UÅ¼yj nowych narzÄ™dzi automatyzacji dla najszybszego uruchomienia:

```bash
# PeÅ‚na konfiguracja projektu (Å›rodowisko + Docker)
./rag.sh setup

# Aktywuj Å›rodowisko
conda activate myenv

# PrzetwÃ³rz dokumenty
./rag.sh pipeline

# Uruchom aplikacjÄ™
./rag.sh app
```

### ğŸ“š SzczegÃ³Å‚owa konfiguracja

#### 1. Przygotowanie Å›rodowiska
Zainstaluj MinicondÄ™ i utwÃ³rz Å›rodowisko z pliku `.yml`:

```bash
conda env create -f environment.yml
conda activate myenv
```

### 2. Uruchomienie Ollama
Kontener Ollama sÅ‚uÅ¼y jako runtime dla modelu Bielik:

```bash
# Pobierz i uruchom kontener Ollama z obsÅ‚ugÄ… GPU
docker run -d --gpus=all -v ${PWD}/ollama_volumes:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# WznÃ³w kontener (przy kolejnych uruchomieniach)
docker start ollama
```

### 3. Konfiguracja i uruchomienie Bielika
```bash
# 1. Pobierz model z Hugging Face:
#    https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct-GGUF/blob/main/Bielik-11B-v2.6-Instruct.Q4_K_M.gguf

# 2. Pobierz plik Modelfile z repozytorium HF i zapisz jako "Modelfile"

# 3. Skopiuj model i Modelfile do kontenera Ollama:
docker cp . ollama:/root/.ollama/Bielik-11B-v2_6-Instruct_Q4_K_M

# 4. UtwÃ³rz model w Ollama:
docker exec -it ollama ollama create Bielik-11B-v2_6-Instruct_Q4_K_M -f /root/.ollama/Bielik-11B-v2_6-Instruct_Q4_K_M/Modelfile

# 5. Uruchom model:
docker exec -it ollama ollama run Bielik-11B-v2_6-Instruct_Q4_K_M
```

> â„¹ï¸ JeÅ›li chcesz zmodyfikowaÄ‡ `Modelfile`, skopiuj go ponownie do kontenera i powtÃ³rz kroki 4â€“5.

### 4. Uruchomienie Qdrant
```bash
# Pobierz najnowszy obraz Qdrant:
docker pull qdrant/qdrant

# Uruchom Qdrant na porcie domyÅ›lnym (6333)
docker run -d -p 6333:6333 qdrant/qdrant
```

### 5. Uruchomienie pipeline i aplikacji uÅ¼ytkownika

#### ğŸ†• UÅ¼ywajÄ…c narzÄ™dzi automatyzacji (zalecane):
```bash
# Przetwarzanie dokumentÃ³w i przygotowanie embeddingÃ³w
./rag.sh pipeline

# Testy systemu i generowanie raportu
./rag.sh test

# Aplikacja uÅ¼ytkownika (czat w Streamlit)
./rag.sh app
```

#### ğŸ“š RÄ™czne uruchomienie:
- **Przetwarzanie dokumentÃ³w i przygotowanie embeddingÃ³w**:
  ```bash
  python rag_pipeline.py
  ```
- **Testy systemu i generowanie raportu**:
  ```bash
  python rag_run_tests.py
  ```
- **Aplikacja uÅ¼ytkownika (czat w Streamlit)**:
  ```bash
  streamlit run rag_user_app.py
  ```

---

## ğŸ› ï¸ Opis dziaÅ‚ania

### `rag_pipeline.py`
- czyÅ›ci dane wejÅ›ciowe,  
- dzieli je na chunki,  
- generuje embeddingi modelem **mmlw-roberta-large**,  
- zapisuje embeddingi i metadane w bazie **Qdrant**,
- generuje encodingi z wykorzystaniem funkcji BM25,
- zapisuje encodingi w bazie danych BM25.

### `rag_run_tests.py`
- uruchamia testy systemu,
- przypadki testowe majÄ… formÄ™ plikÃ³w json:
```json
{
    "question": "pytanie testowe",
    "required_keywords": [
        "lista podstawowych sÅ‚Ã³w kluczowych"
    ],
    "optional_keywords": [
        "lista opcjonalnych sÅ‚Ã³w kluczowych"
    ],
    "expected_answer": "odpowiedÅº wzorcowa"
}
```
- testy wykorzystujÄ… dwie metody:
  - dopasowanie podstawowych i opcjonalnych sÅ‚Ã³w kluczowych,
  - LLM-as-a-judge: model porÃ³wnuje odpowiedÅº wzorcowÄ… z odpowiedziÄ… testowanego systemu.
- na podstawie wynikÃ³w testÃ³w przygotowywany jest raport TEST_REPORT.md

### `rag_user_app.py` (Streamlit)
- udostÄ™pnia prosty interfejs czatu na `localhost`,  
- pobiera pytanie uÅ¼ytkownika, zamienia je na embedding,  
- wysyÅ‚a zapytanie do **Qdrant**, aby odnaleÅºÄ‡ najbardziej pasujÄ…ce fragmenty,  
- dokleja znalezione chunki jako kontekst do promptu,  
- wysyÅ‚a pytanie i kontekst do modelu **Bielik**,  
- wyÅ›wietla odpowiedÅº w formie strumieniowanego czatu.

---

## ğŸ“‚ Struktura projektu

```
rag_pipeline.py                     # logika przetwarzania i przygotowania danych
rag_run_tests.py                    # testy systemu i generowanie raportu
rag_user_app.py                     # interfejs uÅ¼ytkownika (czat w Streamlit)
Makefile                            # ğŸ†• automatyzacja zadaÅ„ projektowych
rag.sh                              # ğŸ†• przyjazny skrypt pomocniczy
MAKEFILE_GUIDE.md                  # ğŸ†• szczegÃ³Å‚owy przewodnik po narzÄ™dziach
common/                             # biblioteki wspÃ³lne: obsÅ‚uga plikÃ³w, Qdrant, Bielik, embeddingi
docs_zip/                           # folder z oryginalnymi spakowanymi plikami
docs/                               # folder z rozpakowanymi plikami
docs_preprocessed/
    â”œâ”€ docs_cleaned_up/             # oczyszczone i ujednolicone pliki
    â””â”€ docs_divided_into_chunks/    # pliki podzielone na chunki
text_chunks/                        # kaÅ¼dy plik = chunk do embeddingu i encodingu
```

---

## ğŸ¤– UÅ¼yte modele

- **Bielik-11B-v2.6-Instruct.Q4_K_M.gguf**  
  [Hugging Face link](https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct-GGUF)

- **mmlw-roberta-large**  
  [Hugging Face link](https://huggingface.co/sdadas/mmlw-roberta-large)

---

## ğŸ“¦ UÅ¼yte kontenery

- **Ollama** â€“ runtime do uruchamiania modelu Bielik,  
- **Qdrant** â€“ wektorowa baza danych do przechowywania embeddingÃ³w i metadanych.

---

## ğŸ†• NarzÄ™dzia automatyzacji

Projekt zawiera zaawansowane narzÄ™dzia automatyzacji, ktÃ³re znacznie przyspieszajÄ… rozwÃ³j:

### ğŸš€ Szybkie komendy
```bash
./rag.sh setup      # PeÅ‚na konfiguracja
./rag.sh start      # Uruchom usÅ‚ugi
./rag.sh status     # SprawdÅº status
./rag.sh pipeline   # PrzetwÃ³rz dokumenty
./rag.sh app        # Uruchom aplikacjÄ™
./rag.sh stop       # Zatrzymaj usÅ‚ugi
```

### ğŸ“š WiÄ™cej informacji
- **Przewodnik po Makefile**: `MAKEFILE_GUIDE.md`
- **Pomoc Makefile**: `make help`
- **Pomoc skryptu**: `./rag.sh help`

### ğŸ¯ KorzyÅ›ci
- âš¡ **Szybszy rozwÃ³j** - jedna komenda zamiast wielu krokÃ³w
- ğŸ”„ **SpÃ³jne procesy** - ustandaryzowane workflow dla zespoÅ‚u
- ğŸ³ **Åatwe zarzÄ…dzanie Docker** - automatyzacja kontenerÃ³w
- ğŸ§ª **Automatyczne testy** - zintegrowane sprawdzanie jakoÅ›ci
- ğŸ“Š **Monitoring** - szybkie sprawdzanie stanu projektu
# üìö RAG QA Chat z Bielikiem i Qdrant

## üéØ Cel projektu
Celem projektu by≈Ço stworzenie kompletnego systemu **RAG (Retrieval-Augmented Generation)**.
System pozwala u≈ºytkownikowi zadawaƒá pytania dotyczƒÖce tre≈õci dokument√≥w, kt√≥re wcze≈õniej zosta≈Çy:

- oczyszczone i ujednolicone,
- podzielone na **chunki**,
- zamienione na **embeddingi** (model *mmlw-roberta-large*) i zapisane w bazie wektorowej **Qdrant**,
- zamienione na encodingi MB25 i zapisane w bazie encoding√≥w.

Na tej podstawie system wyszukuje najbardziej adekwatne fragmenty dokument√≥w i przekazuje je do du≈ºego modelu jƒôzykowego **Bielik-11B-v2.6-Instruct**, kt√≥ry generuje odpowied≈∫ prezentowanƒÖ u≈ºytkownikowi w formie czatu.

---

## üöÄ Jak uruchomiƒá projekt

### 1. Przygotowanie ≈õrodowiska
Zainstaluj Minicondƒô i utw√≥rz ≈õrodowisko z pliku `.yml`:

```bash
conda env create -f environment.yml
conda activate rag_env
```

### 2. Uruchomienie Ollama
Kontener Ollama s≈Çu≈ºy jako runtime dla modelu Bielik:

```bash
# Pobierz i uruchom kontener Ollama z obs≈ÇugƒÖ GPU
docker run -d --gpus=all -v ${PWD}/ollama_volumes:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Wzn√≥w kontener (przy kolejnych uruchomieniach)
docker start ollama
```

### 3. Konfiguracja i uruchomienie Bielika
```bash
# 1. Pobierz model z Hugging Face:
#    https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct-GGUF/blob/main/Bielik-11B-v2.6-Instruct.Q4_K_M.gguf

# 2. Pobierz plik Modelfile z repozytorium HF i zapisz jako "Modelfile"

# 3. Skopiuj model i Modelfile do kontenera Ollama:
docker cp . ollama:/root/.ollama/Bielik-11B-v2_6-Instruct_Q4_K_M

# 4. Utw√≥rz model w Ollama:
docker exec -it ollama ollama create Bielik-11B-v2_6-Instruct_Q4_K_M -f /root/.ollama/Bielik-11B-v2_6-Instruct_Q4_K_M/Modelfile

# 5. Uruchom model:
docker exec -it ollama ollama run Bielik-11B-v2_6-Instruct_Q4_K_M
```

> ‚ÑπÔ∏è Je≈õli chcesz zmodyfikowaƒá `Modelfile`, skopiuj go ponownie do kontenera i powt√≥rz kroki 4‚Äì5.

### 4. Uruchomienie Qdrant
```bash
# Pobierz najnowszy obraz Qdrant:
docker pull qdrant/qdrant

# Uruchom Qdrant na porcie domy≈õlnym (6333)
docker run -d -p 6333:6333 qdrant/qdrant
```

### 5. Uruchomienie pipeline i aplikacji u≈ºytkownika
- **Przetwarzanie dokument√≥w i przygotowanie embedding√≥w**:
  ```bash
  python rag_pipeline.py
  ```
- **Testy systemu i generowanie raportu**:
  ```bash
  python rag_run_tests.py
  ```
- **Aplikacja u≈ºytkownika (czat w Streamlit)**:
  ```bash
  streamlit run rag_user_app.py
  ```

---

## üõ†Ô∏è Opis dzia≈Çania

### `rag_pipeline.py`
- czy≈õci dane wej≈õciowe,  
- dzieli je na chunki,  
- generuje embeddingi modelem **mmlw-roberta-large**,  
- zapisuje embeddingi i metadane w bazie **Qdrant**,
- generuje encodingi z wykorzystaniem funkcji BM25,
- zapisuje encodingi w bazie danych BM25.

### `rag_run_tests.py`
- uruchamia testy systemu,
- przypadki testowe majƒÖ formƒô plik√≥w json:
```json
{
    "question": "pytanie testowe",
    "required_keywords": [
        "lista podstawowych s≈Ç√≥w kluczowych"
    ],
    "optional_keywords": [
        "lista opcjonalnych s≈Ç√≥w kluczowych"
    ],
    "expected_answer": "odpowied≈∫ wzorcowa"
}
```
- testy wykorzystujƒÖ dwie metody:
  - dopasowanie podstawowych i opcjonalnych s≈Ç√≥w kluczowych,
  - LLM-as-a-judge: model por√≥wnuje odpowied≈∫ wzorcowƒÖ z odpowiedziƒÖ testowanego systemu.
- na podstawie wynik√≥w test√≥w przygotowywany jest raport TEST_REPORT.md

### `rag_user_app.py` (Streamlit)
- udostƒôpnia prosty interfejs czatu na `localhost`,  
- pobiera pytanie u≈ºytkownika, zamienia je na embedding,  
- wysy≈Ça zapytanie do **Qdrant**, aby odnale≈∫ƒá najbardziej pasujƒÖce fragmenty,  
- dokleja znalezione chunki jako kontekst do promptu,  
- wysy≈Ça pytanie i kontekst do modelu **Bielik**,  
- wy≈õwietla odpowied≈∫ w formie strumieniowanego czatu.

---

## üìÇ Struktura projektu

```
rag_pipeline.py                     # logika przetwarzania i przygotowania danych
rag_run_tests.py                    # testy systemu i generowanie raportu
rag_user_app.py                     # interfejs u≈ºytkownika (czat w Streamlit)
common/                             # biblioteki wsp√≥lne: obs≈Çuga plik√≥w, Qdrant, Bielik, embeddingi
docs_zip/                           # folder z oryginalnymi spakowanymi plikami
docs/                               # folder z rozpakowanymi plikami
docs_preprocessed/
    ‚îú‚îÄ docs_cleaned_up/             # oczyszczone i ujednolicone pliki
    ‚îî‚îÄ docs_divided_into_chunks/    # pliki podzielone na chunki
text_chunks/                        # ka≈ºdy plik = chunk do embeddingu i encodingu
```

---

## ü§ñ U≈ºyte modele

- **Bielik-11B-v2.6-Instruct.Q4_K_M.gguf**  
  [Hugging Face link](https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct-GGUF)

- **mmlw-roberta-large**  
  [Hugging Face link](https://huggingface.co/sdadas/mmlw-roberta-large)

---

## üì¶ U≈ºyte kontenery

- **Ollama** ‚Äì runtime do uruchamiania modelu Bielik,  
- **Qdrant** ‚Äì wektorowa baza danych do przechowywania embedding√≥w i metadanych.